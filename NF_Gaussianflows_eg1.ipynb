{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "4YS8jTPUdyWe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMcg-bvLdyWw"
   },
   "outputs": [],
   "source": [
    "def Gaussian_integral2(z,mu,sigma,base_sigma,k_dim=3,pi=math.pi):\n",
    "    c=-torch.matmul(sigma.t()[0],sigma.t()[1:].inverse())\n",
    "    h=torch.matmul(z-mu,sigma.t()[1:].inverse())\n",
    "    a=(1+c**2).sum()/(2*base_sigma)\n",
    "    b=(-c*h).sum(1)/base_sigma\n",
    "    m=(-h**2).sum(1)/(2*base_sigma)\n",
    "    return (pi/(a*(2*base_sigma*pi)**k_dim)).sqrt()*(m+b**2/(4*a)).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHflotGXdyW-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Flow\n",
    "# --------------------\n",
    "class PlanarTransform(nn.Module):\n",
    "    def __init__(self, init_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.u = nn.Parameter(torch.randn(1, 2).normal_(0, init_sigma))\n",
    "        self.w = nn.Parameter(torch.randn(1, 2).normal_(0, init_sigma))\n",
    "        self.b = nn.Parameter(torch.randn(1).fill_(0))\n",
    "\n",
    "    def forward(self, x, normalize_u=True):\n",
    "        # allow for a single forward pass over all the transforms in the flows with a Sequential container\n",
    "        if isinstance(x, tuple):\n",
    "            z, sum_log_abs_det_jacobians = x\n",
    "        else:\n",
    "            z, sum_log_abs_det_jacobians = x, 0\n",
    "\n",
    "        # normalize u s.t. w @ u >= -1; sufficient condition for invertibility\n",
    "        u_hat = self.u\n",
    "        if normalize_u:\n",
    "            wtu = (self.w @ self.u.t()).squeeze()\n",
    "            m_wtu = - 1 + torch.log1p(wtu.exp())\n",
    "            u_hat = self.u + (m_wtu - wtu) * self.w / (self.w @ self.w.t())\n",
    "\n",
    "        # compute transform\n",
    "        f_z = z + u_hat * torch.tanh(z @ self.w.t() + self.b)\n",
    "        # compute log_abs_det_jacobian\n",
    "        psi = (1 - torch.tanh(z @ self.w.t() + self.b)**2) @ self.w\n",
    "        det = 1 + psi @ u_hat.t()\n",
    "        log_abs_det_jacobian = torch.log(torch.abs(det) + 1e-6).squeeze()\n",
    "        sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
    "        \n",
    "        return f_z, sum_log_abs_det_jacobians\n",
    "\n",
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, learnable=True):\n",
    "        super().__init__()\n",
    "        self.mu = nn.Parameter(torch.zeros(2)).requires_grad_(learnable)\n",
    "        self.sigma = nn.Parameter(torch.ones(2, 3).normal_(0, 1)).requires_grad_(learnable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.mu + torch.matmul(x,self.sigma.t())\n",
    "        sum_log_abs_det_jacobians = torch.log((((self.sigma@self.sigma.t()).det()).abs())**0.5*Gaussian_integral2(z,self.mu,self.sigma,args.base_sigma))\n",
    "        return z, sum_log_abs_det_jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xblS1QadyXJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Training\n",
    "# --------------------\n",
    "\n",
    "def optimize_flow(base_dist, flow, target_energy_potential, optimizer, args):\n",
    "\n",
    "    # anneal rate for free energy\n",
    "    temp = lambda i: min(1, 0.01 + i/10000)\n",
    "\n",
    "    for i in range(args.start_step, args.n_steps):\n",
    "\n",
    "        # sample base dist\n",
    "        z = base_dist.sample((args.batch_size, )).to(args.device)\n",
    "\n",
    "        # pass through flow:\n",
    "        # 1. compute expected log_prob of data under base dist -- nothing tied to parameters here so irrelevant to grads\n",
    "        base_log_prob = base_dist.log_prob(z)\n",
    "        # 2. compute sum of log_abs_det_jacobian through the flow\n",
    "        zk, sum_log_abs_det_jacobians = flow(z)\n",
    "        # 3. compute expected log_prob of z_k the target_energy potential\n",
    "        p_log_prob = - temp(i) * target_energy_potential(zk)  # p = exp(-potential) ==> p_log_prob = - potential\n",
    "\n",
    "        loss = base_log_prob - sum_log_abs_det_jacobians - args.beta * p_log_prob\n",
    "        loss = loss.mean(0)\n",
    "\n",
    "        # compute loss and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            # display loss\n",
    "            log_qk = base_dist.log_prob(z) - sum_log_abs_det_jacobians\n",
    "            print('{}: step {:5d} / {}; loss {:.3f}; base_log_prob {:.3f}, sum log dets {:.3f}, p_log_prob {:.3f}, max base = {:.3f}; max qk = {:.3f} \\\n",
    "                zk_mean {}, zk_sigma {}; base_mu {}, base_log_sigma {}'.format(\n",
    "                args.target_potential, i, args.n_steps, loss.item(), base_log_prob.mean(0).item(), sum_log_abs_det_jacobians.mean(0).item(),\n",
    "                p_log_prob.mean(0).item(), base_log_prob.exp().max().item(), log_qk.exp().max().item(),\n",
    "                zk.mean(0).cpu().data.numpy(), zk.var(0).sqrt().cpu().data.numpy(),\n",
    "                base_dist.loc.cpu().data.numpy() if not args.learn_base else flow[0].mu.cpu().data.numpy(),\n",
    "                base_dist.covariance_matrix.cpu().diag().data.numpy() if not args.learn_base else flow[0].sigma.cpu().data.numpy()))\n",
    "\n",
    "            # save model\n",
    "            torch.save({'step': i,\n",
    "                        'flow_state': flow.state_dict(),\n",
    "                        'optimizer_state': optimizer.state_dict()},\n",
    "                        os.path.join(args.output_dir, 'model_state_flow_length_{}.pt'.format(args.flow_length)))\n",
    "\n",
    "            # plot and save results\n",
    "            with torch.no_grad():\n",
    "                plot_flow2(base_dist, flow, os.path.join(args.output_dir, 'approximating_flow_step{}.png'.format(i)), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Plotting\n",
    "# --------------------\n",
    "\n",
    "def plot_flow2(base_dist, flow, filename, args):\n",
    "    n = 100\n",
    "    lim = 4\n",
    "    limx=4\n",
    "    fig, axs = plt.subplots(1, 2, subplot_kw={'aspect': 'equal'})\n",
    "\n",
    "    # plot flow-transformed base dist sample and histogram\n",
    "    z = base_dist.sample((10000,))\n",
    "    zk, _ = flow(z)\n",
    "    zk = zk.cpu().data.numpy()\n",
    "    axs[0].scatter(zk[:,0], zk[:,1], s=10, alpha=0.4)\n",
    "    axs[1].hist2d(zk[:,0], zk[:,1], bins=[limx*50,lim*50], range = [[-4,4],[-4,4]], cmap=plt.cm.jet)\n",
    "\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_xlim(-4, 4)\n",
    "        ax.set_ylim(-4, 4)\n",
    "        ax.get_xaxis().set_visible(True)\n",
    "        ax.get_yaxis().set_visible(True)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pjeHSpJdyXR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Plotting\n",
    "# --------------------\n",
    "\n",
    "def plot_flow(base_dist, flow, filename, args):\n",
    "    n = 200\n",
    "    lim = 4\n",
    "    limx=4\n",
    "    z = base_dist.sample((10000,))\n",
    "    fig, axs = plt.subplots(1, 2, subplot_kw={'aspect': 'equal'})\n",
    "    zz = z.cpu().data.numpy()\n",
    "    axs[0].scatter(zz[:,0], zz[:,1], s=10, alpha=0.4)\n",
    "    axs[1].hist2d(zz[:,0], zz[:,1], bins=[limx*50,lim*50], range = [[-4,4],[-4,4]], cmap=plt.cm.jet)\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_xlim(-4, 4)\n",
    "        ax.set_ylim(-4, 4)\n",
    "        ax.get_xaxis().set_visible(True)\n",
    "        ax.get_yaxis().set_visible(True)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename+'_0.png')\n",
    "    plt.close()\n",
    "    # plot target density we're trying to approx\n",
    "    # plot_target_density(u_z, axs[0,0], lim, n)\n",
    "\n",
    "    # plot posterior approx density\n",
    "    # plot_flow_density(base_dist, flow, axs[0,1], lim, n)\n",
    "\n",
    "    # plot flow-transformed base dist sample and histogram\n",
    "    for i in range(1,34):\n",
    "        fig, axs = plt.subplots(1, 2, subplot_kw={'aspect': 'equal'})\n",
    "\n",
    "        zk, _ = flow[0:i](z)\n",
    "        zk = zk.cpu().data.numpy()\n",
    "        axs[0].scatter(zk[:,0], zk[:,1], s=10, alpha=0.4)\n",
    "        axs[1].hist2d(zk[:,0], zk[:,1], bins=[limx*50,lim*50], range = [[-4,4],[-4,4]], cmap=plt.cm.jet)\n",
    "\n",
    "        for ax in plt.gcf().axes:\n",
    "            ax.set_xlim(-4, 4)\n",
    "            ax.set_ylim(-4, 4)\n",
    "            ax.get_xaxis().set_visible(True)\n",
    "            ax.get_yaxis().set_visible(True)\n",
    "            ax.invert_yaxis()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename+'_%d.png'%i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWgGf_pwdyXb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_target_density(u_z, ax,  n=200, output_dir=None):\n",
    "    x1 = torch.linspace(-4, 4, n)\n",
    "    x2 = torch.linspace(-4, 4, n)\n",
    "    xx, yy = torch.meshgrid((x1, x2))\n",
    "    zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze().to(args.device)\n",
    "    xx=xx.cpu()\n",
    "    yy=yy.cpu()\n",
    "    zz=zz.cpu()\n",
    "    ax.pcolormesh(xx, yy, torch.exp(-u_z(zz)).view(n,n).data, cmap=plt.cm.jet)\n",
    "\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_xlim(-4, 4)\n",
    "        ax.set_ylim(-4, 4)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    if output_dir:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'target_potential_density.png'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01dr2i84dyXj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_flow_density(base_dist, flow, ax, range_lim=4, n=100, output_dir=None):\n",
    "    x = torch.linspace(-range_lim, range_lim, n)\n",
    "    xx, yy,hh= torch.meshgrid((x, x, x))\n",
    "    zz = torch.stack((xx.flatten(), yy.flatten(),hh.flatten()), dim=-1).squeeze().to(args.device)\n",
    "    # plot posterior approx density\n",
    "    zzk, sum_log_abs_det_jacobians = flow(zz)\n",
    "    log_q0 = base_dist.log_prob(zz)\n",
    "    log_qk = log_q0 - sum_log_abs_det_jacobians\n",
    "    qk = log_qk.exp().cpu()\n",
    "    zzk = zzk.cpu()\n",
    "    n1=1000\n",
    "    ax.pcolormesh(zzk[:,0].view(n1,n1).data, zzk[:,1].view(n1,n1).data, qk.view(n1,n1).data, cmap=plt.cm.jet)\n",
    "    ax.set_facecolor(plt.cm.jet(0.))\n",
    "\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_xlim(-range_lim, range_lim)\n",
    "        ax.set_ylim(-range_lim, range_lim)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    if output_dir:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'flow_k{}_density.png'.format(len(flow)-1)))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LCoRlhGdyXr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w1 = lambda z: torch.sin(2 * math.pi * z[:,0] / 4)\n",
    "w2 = lambda z: 3 * torch.exp(-0.5 * ((z[:,0] - 1)/0.6)**2)\n",
    "w3 = lambda z: 3 * torch.sigmoid((z[:,0] - 1) / 0.3)\n",
    "\n",
    "u_z1 = lambda z: 0.5 * ((torch.norm(z, p=2, dim=1) - 2) / 0.4)**2 - \\\n",
    "                 torch.log(torch.exp(-0.5*((z[:,0] - 2) / 0.6)**2) + torch.exp(-0.5*((z[:,0] + 2) / 0.6)**2) + 1e-10)\n",
    "u_z2 = lambda z: 0.5 * ((z[:,1] - w1(z)) / 0.4)**2\n",
    "u_z3 = lambda z: - torch.log(torch.exp(-0.5*((z[:,1] - w1(z))/0.35)**2) + torch.exp(-0.5*((z[:,1] - w1(z) + w2(z))/0.35)**2) + 1e-10)\n",
    "u_z4 = lambda z: - torch.log(torch.exp(-0.5*((z[:,1] - w1(z))/0.4)**2) + torch.exp(-0.5*((z[:,1] - w1(z) + w3(z))/0.35)**2) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viOK1tpodyXw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Run\n",
    "# --------------------\n",
    "class args:\n",
    "    # actions\n",
    "    use_cuda=True                                          # whether to use cuda \n",
    "    restore_file=0 # Path to model to restore.\n",
    "    output_dir='./results_test_u_z1_k8' # Path to output folder.\n",
    "    train=True # Train a flow.\n",
    "    evaluate=True # Evaluate a flow\n",
    "    plot=True # Plot a flow and target density.\n",
    "\n",
    "    # flow params\n",
    "    learn_base=True # Whether to learn a mu-sigma affine transform of the base distribution.\n",
    "    flow_length=8 # Length of the flow.\n",
    "    base_sigma=1.0 # Std of the base isotropic 0-mean Gaussian distribution.\n",
    "\n",
    "    # target potential\n",
    "    target_potential='u_z1' # Which potential function to approximate.\n",
    "\n",
    "    # training params\n",
    "    seed=2 # Random seed\n",
    "    init_sigma=1.0#Initialization std for the trainable flow parameters.\n",
    "    batch_size=100 # batch size in training\n",
    "    start_step=0 # Starting step (if resuming training will be overwrite from filename).\n",
    "    n_steps=1000000 # Optimization steps.\n",
    "    lr= 1e-5 # Learning rate.\n",
    "    weight_decay= 1e-3 # Weight decay.\n",
    "    beta=1.0 # Multiplier for the target potential loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sT0lQpbCdyX5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# cuda to device\n",
    "args.device = torch.device('cuda:0' if torch.cuda.is_available() and args.use_cuda else 'cpu')\n",
    "\n",
    "# set up random seed\n",
    "torch.manual_seed(args.seed)\n",
    "if args.device.type == 'cuda': torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# setup flow\n",
    "flow = nn.Sequential(AffineTransform(args.learn_base), *[PlanarTransform(args.init_sigma) for _ in range(args.flow_length)]).to(args.device)\n",
    "\n",
    "# setup target potential to approx\n",
    "u_z = vars()[args.target_potential]\n",
    "\n",
    "# setup base distribution\n",
    "base_dist = D.MultivariateNormal(torch.zeros(3).to(args.device), args.base_sigma * torch.eye(3).to(args.device))\n",
    "\n",
    "if args.restore_file:\n",
    "    # get filename\n",
    "    filename = os.path.basename(args.restore_file)\n",
    "    args.flow_length = int(filename.partition('length_')[-1].rpartition('.')[0])\n",
    "    # reset output dir\n",
    "    args.output_dir = os.path.dirname(args.restore_file)\n",
    "    # load state\n",
    "    state = torch.load(args.restore_file, map_location=args.device)\n",
    "    # compatibility code;\n",
    "    # 1/ earlier models did not include step and optimizer checkpoints;\n",
    "    try:\n",
    "        flow_state = state['flow_state']\n",
    "        optimizer_state = state['optimizer_state']\n",
    "        args.start_step = state['step']\n",
    "    except KeyError:\n",
    "        # if state is not a dict, load just the model state\n",
    "        flow_state = state\n",
    "        optimizer_state = None\n",
    "    # 2/ some saved checkpoints may not have a first affine layer\n",
    "    try:\n",
    "        flow_state['0.mu']\n",
    "    except KeyError:\n",
    "        # if no first affine layer, reload a flow model without one\n",
    "        flow = nn.Sequential(*[PlanarTransform(args.init_sigma) for _ in range(args.flow_length)])\n",
    "    flow.load_state_dict(flow_state)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1zzhNYMdyYA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(args.output_dir):\n",
    "    os.makedirs(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W7LxAktgdyYI",
    "outputId": "f6dfdd3f-6e84-430b-c183-c19fc7328d53",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_z1: step     0 / 1000000; loss 2.197; base_log_prob -4.281, sum log dets -6.380, p_log_prob -0.098, max base = 0.062; max qk = 50.293                 zk_mean [ 0.00970835 -0.05864723], zk_sigma [0.88021344 0.90119123]; base_mu [3.1613799e-05 3.1622294e-05], base_log_sigma [[ 0.39232844 -0.22359563 -0.3195319 ]\n",
      " [-1.2050687   1.0444951  -0.63325936]]\n",
      "u_z1: step 10000 / 1000000; loss 0.965; base_log_prob -4.215, sum log dets -3.381, p_log_prob -1.799, max base = 0.059; max qk = 1.373                 zk_mean [-0.43799418 -0.3655107 ], zk_sigma [1.7723008 1.1493797]; base_mu [0.04256683 0.00590959], base_log_sigma [[ 0.01968786 -0.27891803 -0.41128987]\n",
      " [-0.80275464  1.2233868  -0.8458111 ]]\n",
      "u_z1: step 20000 / 1000000; loss 0.113; base_log_prob -4.335, sum log dets -3.639, p_log_prob -0.808, max base = 0.062; max qk = 1.480                 zk_mean [-0.41184798 -0.3405348 ], zk_sigma [1.859276  1.0698439]; base_mu [0.08920507 0.02804931], base_log_sigma [[-0.05466025 -0.1107948  -0.38637984]\n",
      " [-0.5704993   1.1928636  -0.8507302 ]]\n",
      "u_z1: step 30000 / 1000000; loss 0.115; base_log_prob -4.201, sum log dets -3.415, p_log_prob -0.900, max base = 0.063; max qk = 1.399                 zk_mean [-0.1801064   0.01540608], zk_sigma [1.8361706 1.2082772]; base_mu [0.05633517 0.04927663], base_log_sigma [[-0.05667698 -0.07958637 -0.41164204]\n",
      " [-0.45167178  1.1553427  -0.870653  ]]\n",
      "u_z1: step 40000 / 1000000; loss -0.005; base_log_prob -4.397, sum log dets -3.508, p_log_prob -0.885, max base = 0.060; max qk = 1.348                 zk_mean [-0.27949202 -0.17063163], zk_sigma [1.867276  1.1320276]; base_mu [0.03278841 0.05299791], base_log_sigma [[-0.05579232 -0.04482128 -0.41273218]\n",
      " [-0.38183445  1.1295692  -0.88636476]]\n",
      "u_z1: step 50000 / 1000000; loss -0.022; base_log_prob -4.201, sum log dets -3.229, p_log_prob -0.949, max base = 0.061; max qk = 1.306                 zk_mean [-0.02395551  0.06767748], zk_sigma [1.80213   1.1632196]; base_mu [0.03847341 0.07361197], base_log_sigma [[-0.05535191 -0.02535606 -0.41532478]\n",
      " [-0.3276933   1.1098154  -0.8930995 ]]\n",
      "u_z1: step 60000 / 1000000; loss 0.194; base_log_prob -4.355, sum log dets -3.645, p_log_prob -0.904, max base = 0.061; max qk = 1.451                 zk_mean [-0.19245727 -0.21294768], zk_sigma [1.7919328 1.1622212]; base_mu [0.02870082 0.07156496], base_log_sigma [[-0.04998515 -0.01413945 -0.4219356 ]\n",
      " [-0.28437203  1.0967262  -0.9001007 ]]\n",
      "u_z1: step 70000 / 1000000; loss 0.080; base_log_prob -4.162, sum log dets -3.553, p_log_prob -0.690, max base = 0.061; max qk = 1.195                 zk_mean [-0.20346005 -0.00686849], zk_sigma [1.8975302 1.03611  ]; base_mu [0.01616547 0.06434164], base_log_sigma [[-0.0492567  -0.01309508 -0.43267834]\n",
      " [-0.24845643  1.0699205  -0.920728  ]]\n",
      "u_z1: step 80000 / 1000000; loss -0.056; base_log_prob -4.268, sum log dets -3.503, p_log_prob -0.708, max base = 0.058; max qk = 1.199                 zk_mean [-0.13308263  0.02959107], zk_sigma [1.9181855 0.9159405]; base_mu [0.02243157 0.07218063], base_log_sigma [[-0.0504218  -0.01365935 -0.4362504 ]\n",
      " [-0.24129255  1.0421851  -0.93028307]]\n",
      "u_z1: step 90000 / 1000000; loss 0.008; base_log_prob -4.364, sum log dets -3.449, p_log_prob -0.922, max base = 0.060; max qk = 1.155                 zk_mean [-0.31648627  0.06872804], zk_sigma [1.859283 1.181101]; base_mu [0.01215435 0.06636097], base_log_sigma [[-0.03875255 -0.01746183 -0.4493721 ]\n",
      " [-0.19362268  1.0208558  -0.94830596]]\n",
      "u_z1: step 100000 / 1000000; loss -0.086; base_log_prob -4.218, sum log dets -3.378, p_log_prob -0.753, max base = 0.059; max qk = 1.209                 zk_mean [-0.5621678  0.1137718], zk_sigma [1.8409531 1.1223154]; base_mu [0.02847442 0.08526155], base_log_sigma [[-0.04150755 -0.02370287 -0.45550305]\n",
      " [-0.179625    0.9912693  -0.9599421 ]]\n",
      "u_z1: step 110000 / 1000000; loss 0.165; base_log_prob -4.182, sum log dets -3.378, p_log_prob -0.969, max base = 0.057; max qk = 1.148                 zk_mean [-0.14266323 -0.17755525], zk_sigma [1.9052782 1.1305647]; base_mu [0.02806204 0.08739688], base_log_sigma [[-0.03527722 -0.0238344  -0.4581187 ]\n",
      " [-0.14913782  0.9713307  -0.9705804 ]]\n",
      "u_z1: step 120000 / 1000000; loss -0.301; base_log_prob -4.190, sum log dets -3.242, p_log_prob -0.647, max base = 0.060; max qk = 1.142                 zk_mean [0.06625044 0.35671794], zk_sigma [1.8488421 1.1051983]; base_mu [0.03285684 0.09515246], base_log_sigma [[-0.03506235 -0.02588122 -0.45855647]\n",
      " [-0.1305449   0.94565713 -0.9774462 ]]\n",
      "u_z1: step 130000 / 1000000; loss -0.184; base_log_prob -4.115, sum log dets -3.284, p_log_prob -0.647, max base = 0.062; max qk = 1.199                 zk_mean [-0.236965    0.01559822], zk_sigma [1.8665894  0.99774224]; base_mu [0.02287984 0.08993692], base_log_sigma [[-0.03019486 -0.01793809 -0.4490948 ]\n",
      " [-0.12118414  0.9407837  -0.96881795]]\n",
      "u_z1: step 140000 / 1000000; loss -0.003; base_log_prob -4.366, sum log dets -3.370, p_log_prob -0.993, max base = 0.058; max qk = 1.267                 zk_mean [ 0.08365779 -0.00745663], zk_sigma [1.7634065 1.2385811]; base_mu [0.02484565 0.09381887], base_log_sigma [[-0.03083308 -0.02894332 -0.45215288]\n",
      " [-0.12087489  0.91051096 -0.98084056]]\n",
      "u_z1: step 150000 / 1000000; loss -0.086; base_log_prob -4.127, sum log dets -3.345, p_log_prob -0.695, max base = 0.062; max qk = 1.118                 zk_mean [-0.17410776  0.02959251], zk_sigma [1.924909  1.1214299]; base_mu [0.02269573 0.09166247], base_log_sigma [[-0.03039996 -0.04298778 -0.463072  ]\n",
      " [-0.1208111   0.87209433 -1.0006076 ]]\n",
      "u_z1: step 160000 / 1000000; loss 0.223; base_log_prob -4.403, sum log dets -3.586, p_log_prob -1.040, max base = 0.063; max qk = 1.197                 zk_mean [-0.23651601  0.16156287], zk_sigma [1.7996897 1.2776488]; base_mu [0.03217258 0.10203839], base_log_sigma [[-0.02700779 -0.04876801 -0.46438214]\n",
      " [-0.10649176  0.8434782  -1.008845  ]]\n",
      "u_z1: step 170000 / 1000000; loss 0.004; base_log_prob -4.279, sum log dets -3.480, p_log_prob -0.802, max base = 0.058; max qk = 1.253                 zk_mean [-0.11271114  0.09421033], zk_sigma [1.8600278 1.1318871]; base_mu [0.02072701 0.09178375], base_log_sigma [[-0.02738086 -0.06072065 -0.47513348]\n",
      " [-0.11140418  0.81063884 -1.0224891 ]]\n",
      "u_z1: step 180000 / 1000000; loss 0.052; base_log_prob -4.497, sum log dets -3.624, p_log_prob -0.925, max base = 0.058; max qk = 1.280                 zk_mean [-0.02263928  0.08807255], zk_sigma [1.8806286 1.1517316]; base_mu [0.0167124 0.0865047], base_log_sigma [[-0.02886038 -0.06455766 -0.46936086]\n",
      " [-0.11071718  0.7910019  -1.0219455 ]]\n",
      "u_z1: step 190000 / 1000000; loss 0.008; base_log_prob -4.352, sum log dets -3.548, p_log_prob -0.812, max base = 0.057; max qk = 1.224                 zk_mean [ 0.17459089 -0.13832967], zk_sigma [1.9011188 1.0586219]; base_mu [0.00997933 0.08119497], base_log_sigma [[-0.0251538  -0.05497557 -0.45358583]\n",
      " [-0.0955908   0.79026914 -1.0092899 ]]\n",
      "u_z1: step 200000 / 1000000; loss 0.368; base_log_prob -4.566, sum log dets -3.817, p_log_prob -1.117, max base = 0.060; max qk = 1.396                 zk_mean [-0.34793153  0.07038625], zk_sigma [1.7337153 1.1740221]; base_mu [0.00421667 0.07593841], base_log_sigma [[-0.03112427 -0.06276993 -0.45465702]\n",
      " [-0.10511467  0.7695365  -1.0119175 ]]\n",
      "u_z1: step 210000 / 1000000; loss -0.185; base_log_prob -4.216, sum log dets -3.239, p_log_prob -0.791, max base = 0.060; max qk = 1.123                 zk_mean [ 0.03263266 -0.28184062], zk_sigma [1.9223286 1.0084956]; base_mu [0.00614837 0.07840771], base_log_sigma [[-0.02508734 -0.07517503 -0.46467808]\n",
      " [-0.09257691  0.74012244 -1.0254344 ]]\n",
      "u_z1: step 220000 / 1000000; loss -0.081; base_log_prob -4.143, sum log dets -3.355, p_log_prob -0.707, max base = 0.061; max qk = 1.289                 zk_mean [-0.18266004  0.18474327], zk_sigma [1.8695955 1.0232689]; base_mu [0.00309289 0.0747027 ], base_log_sigma [[-0.02304287 -0.0863191  -0.46894675]\n",
      " [-0.08519211  0.7150285  -1.033279  ]]\n",
      "u_z1: step 230000 / 1000000; loss -0.065; base_log_prob -4.316, sum log dets -3.430, p_log_prob -0.822, max base = 0.056; max qk = 1.227                 zk_mean [-0.18078296  0.03027069], zk_sigma [1.9149879 1.162692 ]; base_mu [-0.00511535  0.06690472], base_log_sigma [[-0.01604999 -0.0995385  -0.48035818]\n",
      " [-0.06859025  0.6884532  -1.0464784 ]]\n"
     ]
    }
   ],
   "source": [
    "if args.train:\n",
    "    optimizer = torch.optim.RMSprop(flow.parameters(), lr=args.lr, momentum=0.9, alpha=0.90, eps=1e-6, weight_decay=args.weight_decay)\n",
    "    if args.restore_file and optimizer_state:\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    args.n_steps = args.start_step + args.n_steps\n",
    "    optimize_flow(base_dist, flow, u_z, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pRrvbnddyYV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.evaluate:\n",
    "    plot_flow(base_dist, flow, os.path.join(args.output_dir, 'approximating_flow'), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD8ZT4uOdyYe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.plot:\n",
    "    plot_target_density(u_z, plt.gca(), output_dir=args.output_dir)\n",
    "    plot_flow_density(base_dist, flow, plt.gca(), output_dir=args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9L9IhOndyYj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NF_Planar_Flows_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
