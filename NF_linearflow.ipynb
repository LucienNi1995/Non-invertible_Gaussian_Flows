{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":false},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.distributions as D\nimport math\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport os","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# --------------------\n# Flow\n# --------------------\nclass PlanarTransform(nn.Module):\n    def __init__(self, init_sigma=1.0):\n        super().__init__()\n        self.u = nn.Parameter(torch.randn(1, 2).normal_(0, init_sigma))\n        self.w = nn.Parameter(torch.randn(1, 2).normal_(0, init_sigma))\n        self.b = nn.Parameter(torch.randn(1).fill_(0))\n\n    def forward(self, x, normalize_u=True):\n        # allow for a single forward pass over all the transforms in the flows with a Sequential container\n        if isinstance(x, tuple):\n            z, sum_log_abs_det_jacobians = x\n        else:\n            z, sum_log_abs_det_jacobians = x, 0\n\n        # normalize u s.t. w @ u >= -1; sufficient condition for invertibility\n        u_hat = self.u\n        if normalize_u:\n            wtu = (self.w @ self.u.t()).squeeze()\n            m_wtu = - 1 + torch.log1p(wtu.exp())\n            u_hat = self.u + (m_wtu - wtu) * self.w / (self.w @ self.w.t())\n\n        # compute transform\n        f_z = z + u_hat * torch.tanh(z @ self.w.t() + self.b)\n        # compute log_abs_det_jacobian\n        psi = (1 - torch.tanh(z @ self.w.t() + self.b)**2) @ self.w\n        det = 1 + psi @ u_hat.t()\n        log_abs_det_jacobian = torch.log(torch.abs(det) + 1e-6).squeeze()\n        sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n\n        return f_z, sum_log_abs_det_jacobians\n\nclass AffineTransform(nn.Module):\n    def __init__(self, learnable=True):\n        super().__init__()\n        self.mu = nn.Parameter(torch.zeros(2)).requires_grad_(learnable)\n        self.sigma = nn.Parameter(torch.ones(2,2).normal_(0,1)).requires_grad_(learnable)\n\n    def forward(self, x):\n        z = self.mu + torch.matmul(x,self.sigma.t())\n        sum_log_abs_det_jacobians = torch.log(self.sigma.det())\n        return z, sum_log_abs_det_jacobians","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# --------------------\n# Training\n# --------------------\n\ndef optimize_flow(base_dist, flow, target_energy_potential, optimizer, args):\n\n    # anneal rate for free energy\n    temp = lambda i: min(1, 0.01 + i/10000)\n\n    for i in range(args.start_step, args.n_steps):\n\n        # sample base dist\n        z = base_dist.sample((args.batch_size, )).to(args.device)\n\n        # pass through flow:\n        # 1. compute expected log_prob of data under base dist -- nothing tied to parameters here so irrelevant to grads\n        base_log_prob = base_dist.log_prob(z)\n        # 2. compute sum of log_abs_det_jacobian through the flow\n        zk, sum_log_abs_det_jacobians = flow(z)\n        # 3. compute expected log_prob of z_k the target_energy potential\n        p_log_prob = - temp(i) * target_energy_potential(zk)  # p = exp(-potential) ==> p_log_prob = - potential\n\n        loss = base_log_prob - sum_log_abs_det_jacobians - args.beta * p_log_prob\n        loss = loss.mean(0)\n\n        # compute loss and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 10000 == 0:\n            # display loss\n            log_qk = base_dist.log_prob(z) - sum_log_abs_det_jacobians\n            print('{}: step {:5d} / {}; loss {:.3f}; base_log_prob {:.3f}, sum log dets {:.3f}, p_log_prob {:.3f}, max base = {:.3f}; max qk = {:.3f} \\\n                zk_mean {}, zk_sigma {}; base_mu {}, base_log_sigma {}'.format(\n                args.target_potential, i, args.n_steps, loss.item(), base_log_prob.mean(0).item(), sum_log_abs_det_jacobians.mean(0).item(),\n                p_log_prob.mean(0).item(), base_log_prob.exp().max().item(), log_qk.exp().max().item(),\n                zk.mean(0).cpu().data.numpy(), zk.var(0).sqrt().cpu().data.numpy(),\n                base_dist.loc.cpu().data.numpy() if not args.learn_base else flow[0].mu.cpu().data.numpy(),\n                base_dist.covariance_matrix.cpu().diag().data.numpy() if not args.learn_base else flow[0].logsigma.cpu().data.numpy()))\n\n            # save model\n            torch.save({'step': i,\n                        'flow_state': flow.state_dict(),\n                        'optimizer_state': optimizer.state_dict()},\n                        os.path.join(args.output_dir, 'model_state_flow_length_{}.pt'.format(args.flow_length)))\n\n            # plot and save results\n            with torch.no_grad():\n                plot_flow2(base_dist, flow, os.path.join(args.output_dir, 'approximating_flow_step{}.png'.format(i)), args)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# --------------------\n# Plotting\n# --------------------\n\ndef plot_flow2(base_dist, flow, filename, args):\n    n = 100\n    lim = 4\n    limx=4\n    fig, axs = plt.subplots(1, 2, subplot_kw={'aspect': 'equal'})\n\n    # plot flow-transformed base dist sample and histogram\n    z = base_dist.sample((10000,))\n    zk, _ = flow(z)\n    zk = zk.cpu().data.numpy()\n    axs[0].scatter(zk[:,0], zk[:,1], s=10, alpha=0.4)\n    axs[1].hist2d(zk[:,0], zk[:,1], bins=[limx*50,lim*50], range = [[-4,4],[-4,4]], cmap=plt.cm.jet)\n\n    for ax in plt.gcf().axes:\n        ax.set_xlim(-4, 4)\n        ax.set_ylim(-4, 4)\n        ax.get_xaxis().set_visible(True)\n        ax.get_yaxis().set_visible(True)\n        ax.invert_yaxis()\n\n    plt.tight_layout()\n    plt.savefig(filename)\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# --------------------\n# Plotting\n# --------------------\n\ndef plot_flow(base_dist, flow, filename, args):\n    n = 200\n    lim = 4\n    limx=4\n    z = base_dist.sample((10000,))\n    fig, axs = plt.subplots(1, 2, subplot_kw={'aspect': 'equal'})\n    zz = z.cpu().data.numpy()\n    axs[0].scatter(zz[:,0], zz[:,1], s=10, alpha=0.4)\n    axs[1].hist2d(zz[:,0], zz[:,1], bins=[limx*50,lim*50], range = [[-4,4],[-4,4]], cmap=plt.cm.jet)\n    for ax in plt.gcf().axes:\n        ax.set_xlim(-4, 4)\n        ax.set_ylim(-4, 4)\n        ax.get_xaxis().set_visible(True)\n        ax.get_yaxis().set_visible(True)\n        ax.invert_yaxis()\n\n    plt.tight_layout()\n    plt.savefig(filename+'_0.png')\n    plt.close()\n    # plot target density we're trying to approx\n    # plot_target_density(u_z, axs[0,0], lim, n)\n\n    # plot posterior approx density\n    # plot_flow_density(base_dist, flow, axs[0,1], lim, n)\n\n    # plot flow-transformed base dist sample and histogram\n    for i in range(1,34):\n        fig, axs = plt.subplots(1, 2, subplot_kw={'aspect': 'equal'})\n\n        zk, _ = flow[0:i](z)\n        zk = zk.cpu().data.numpy()\n        axs[0].scatter(zk[:,0], zk[:,1], s=10, alpha=0.4)\n        axs[1].hist2d(zk[:,0], zk[:,1], bins=[limx*50,lim*50], range = [[-4,4],[-4,4]], cmap=plt.cm.jet)\n\n        for ax in plt.gcf().axes:\n            ax.set_xlim(-4, 4)\n            ax.set_ylim(-4, 4)\n            ax.get_xaxis().set_visible(True)\n            ax.get_yaxis().set_visible(True)\n            ax.invert_yaxis()\n\n        plt.tight_layout()\n        plt.savefig(filename+'_%d.png'%i)\n        plt.close()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def plot_target_density(u_z, ax,  n=200, output_dir=None):\n    x1 = torch.linspace(-4, 4, n)\n    x2 = torch.linspace(-4, 4, n)\n    xx, yy = torch.meshgrid((x1, x2))\n    zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze().to(args.device)\n    xx=xx.cpu()\n    yy=yy.cpu()\n    zz=zz.cpu()\n    ax.pcolormesh(xx, yy, torch.exp(-u_z(zz)).view(n,n).data, cmap=plt.cm.jet)\n\n    for ax in plt.gcf().axes:\n        ax.set_xlim(-4, 4)\n        ax.set_ylim(-4, 4)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        ax.invert_yaxis()\n\n    if output_dir:\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'target_potential_density.png'))\n        plt.close()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def plot_flow_density(base_dist, flow, ax, range_lim=4, n=200, output_dir=None):\n    x = torch.linspace(-range_lim, range_lim, n)\n    xx, yy = torch.meshgrid((x, x))\n    zz = torch.stack((xx.flatten(), yy.flatten()), dim=-1).squeeze().to(args.device)\n\n    # plot posterior approx density\n    zzk, sum_log_abs_det_jacobians = flow(zz)\n    log_q0 = base_dist.log_prob(zz)\n    log_qk = log_q0 - sum_log_abs_det_jacobians\n    qk = log_qk.exp().cpu()\n    zzk = zzk.cpu()\n    ax.pcolormesh(zzk[:,0].view(n,n).data, zzk[:,1].view(n,n).data, qk.view(n,n).data, cmap=plt.cm.jet)\n    ax.set_facecolor(plt.cm.jet(0.))\n\n    for ax in plt.gcf().axes:\n        ax.set_xlim(-range_lim, range_lim)\n        ax.set_ylim(-range_lim, range_lim)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        ax.invert_yaxis()\n\n    if output_dir:\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, 'flow_k{}_density.png'.format(len(flow)-1)))\n        plt.close()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"w1 = lambda z: torch.sin(2 * math.pi * z[:,0] / 4)\nw2 = lambda z: 3 * torch.exp(-0.5 * ((z[:,0] - 1)/0.6)**2)\nw3 = lambda z: 3 * torch.sigmoid((z[:,0] - 1) / 0.3)\n\nu_z1 = lambda z: 0.5 * ((torch.norm(z, p=2, dim=1) - 2) / 0.4)**2 - \\\n                 torch.log(torch.exp(-0.5*((z[:,0] - 2) / 0.6)**2) + torch.exp(-0.5*((z[:,0] + 2) / 0.6)**2) + 1e-10)\nu_z2 = lambda z: 0.5 * ((z[:,1] - w1(z)) / 0.4)**2\nu_z3 = lambda z: - torch.log(torch.exp(-0.5*((z[:,1] - w1(z))/0.35)**2) + torch.exp(-0.5*((z[:,1] - w1(z) + w2(z))/0.35)**2) + 1e-10)\nu_z4 = lambda z: - torch.log(torch.exp(-0.5*((z[:,1] - w1(z))/0.4)**2) + torch.exp(-0.5*((z[:,1] - w1(z) + w3(z))/0.35)**2) + 1e-10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# --------------------\n# Run\n# --------------------\nclass args:\n    # actions\n    use_cuda=True                                          # whether to use cuda \n    restore_file=0 # Path to model to restore.\n    output_dir='./results_u_z1_k8' # Path to output folder.\n    train=True # Train a flow.\n    evaluate=True # Evaluate a flow\n    plot=True # Plot a flow and target density.\n\n    # flow params\n    learn_base=True # Whether to learn a mu-sigma affine transform of the base distribution.\n    flow_length=8 # Length of the flow.\n    base_sigma=1.0 # Std of the base isotropic 0-mean Gaussian distribution.\n\n    # target potential\n    target_potential='u_z1' # Which potential function to approximate.\n\n    # training params\n    seed=2 # Random seed\n    init_sigma=1.0 #Initialization std for the trainable flow parameters.\n    batch_size=100 # batch size in training\n    start_step=0 # Starting step (if resuming training will be overwrite from filename).\n    n_steps=1000000 # Optimization steps.\n    lr= 1e-5 # Learning rate.\n    weight_decay= 1e-3 # Weight decay.\n    beta=1.0 # Multiplier for the target potential loss.","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"\n# cuda to device\nargs.device = torch.device('cuda:0' if torch.cuda.is_available() and args.use_cuda else 'cpu')\n\n# set up random seed\ntorch.manual_seed(args.seed)\nif args.device.type == 'cuda': torch.cuda.manual_seed(args.seed)\n\n# setup flow\nflow = nn.Sequential(AffineTransform(args.learn_base), *[PlanarTransform(args.init_sigma) for _ in range(args.flow_length)]).to(args.device)\n\n# setup target potential to approx\nu_z = vars()[args.target_potential]\n\n# setup base distribution\nbase_dist = D.MultivariateNormal(torch.zeros(2).to(args.device), args.base_sigma * torch.eye(2).to(args.device))\n\nif args.restore_file:\n    # get filename\n    filename = os.path.basename(args.restore_file)\n    args.flow_length = int(filename.partition('length_')[-1].rpartition('.')[0])\n    # reset output dir\n    args.output_dir = os.path.dirname(args.restore_file)\n    # load state\n    state = torch.load(args.restore_file, map_location=args.device)\n    # compatibility code;\n    # 1/ earlier models did not include step and optimizer checkpoints;\n    try:\n        flow_state = state['flow_state']\n        optimizer_state = state['optimizer_state']\n        args.start_step = state['step']\n    except KeyError:\n        # if state is not a dict, load just the model state\n        flow_state = state\n        optimizer_state = None\n    # 2/ some saved checkpoints may not have a first affine layer\n    try:\n        flow_state['0.mu']\n    except KeyError:\n        # if no first affine layer, reload a flow model without one\n        flow = nn.Sequential(*[PlanarTransform(args.init_sigma) for _ in range(args.flow_length)])\n    flow.load_state_dict(flow_state)  ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"if not os.path.isdir(args.output_dir):\n    os.makedirs(args.output_dir)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"if args.train:\n    optimizer = torch.optim.RMSprop(flow.parameters(), lr=args.lr, momentum=0.9, alpha=0.90, eps=1e-6, weight_decay=args.weight_decay)\n    if args.restore_file and optimizer_state:\n        optimizer.load_state_dict(optimizer_state)\n    args.n_steps = args.start_step + args.n_steps\n    optimize_flow(base_dist, flow, u_z, optimizer, args)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"if args.evaluate:\n    plot_flow(base_dist, flow, os.path.join(args.output_dir, 'approximating_flow'), args)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"if args.plot:\n    plot_target_density(u_z, plt.gca(), output_dir=args.output_dir)\n    plot_flow_density(base_dist, flow, plt.gca(), output_dir=args.output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}